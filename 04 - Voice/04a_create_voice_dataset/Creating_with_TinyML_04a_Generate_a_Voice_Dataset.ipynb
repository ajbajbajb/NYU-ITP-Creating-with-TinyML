{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Creating with TinyML - 04a. Generate a Voice Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI9NEofM6BE6"
      },
      "source": [
        "# Creating with TinyML - 04a. Generate a Voice Dataset\n",
        "This Colab will download the [speech commands dataset](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) (published by Google) and let you select a subset of words / speech commands to extract. It will then generate a balanced dataset with classes for your selected words. \n",
        "\n",
        "In addition a \"silence\" (ambient noise) class and a \"unknown\" class (random words not in your list)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiUkMNZfStig"
      },
      "source": [
        "## Setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X7V1P29Nxnd"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import shutil\n",
        "import math\n",
        "import soundfile as sf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h27BLr0JSyXw"
      },
      "source": [
        "## Constants, do not change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL6aEfZDLcEE"
      },
      "source": [
        "#CONSTANTS, DO NOT CHANGE\n",
        "DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "OUT_DIR = 'dataset'\n",
        "DOWNLOAD_DIR = 'download/'\n",
        "\n",
        "ALL_WORDS = [\"bed\", \"bird\", \"cat\", \"dog\", \"down\", \"eight\", \"five\", \"follow\", \"forward\", \"four\", \"go\", \"happy\", \"house\", \"learn\", \"left\", \"marvin\", \"nine\", \"no\", \"off\", \"on\", \"one\", \"right\", \"seven\", \"sheila\", \"six\", \"stop\", \"three\", \"tree\", \"two\", \"up\", \"visual\", \"wow\", \"yes\", \"zero\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7H6RK-OS6TM"
      },
      "source": [
        "# Configure\n",
        "\n",
        "Select one or more of the following words:\n",
        "\n",
        "* bed\n",
        "* bird\n",
        "* cat\n",
        "* dog\n",
        "* down\n",
        "* eight\n",
        "* five\n",
        "* follow\n",
        "* forward\n",
        "* four\n",
        "* go\n",
        "* happy\n",
        "* house\n",
        "* learn\n",
        "* left\n",
        "* marvin\n",
        "* nine\n",
        "* no\n",
        "* off\n",
        "* on\n",
        "* one\n",
        "* right\n",
        "* seven\n",
        "* sheila\n",
        "* six\n",
        "* stop\n",
        "* three\n",
        "* tree\n",
        "* two\n",
        "* up\n",
        "* visual\n",
        "* wow\n",
        "* yes\n",
        "* zero\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW__mpamLxm0"
      },
      "source": [
        "\n",
        "# Select here\n",
        "SELECTED_WORDS = ['cat','dog']\n",
        "# Limit the amount of files generated\n",
        "MAX_FILES = 3000\n",
        "\n",
        "# Check our selection against allowed words\n",
        "for label in SELECTED_WORDS:\n",
        "  if not label in ALL_WORDS:\n",
        "    raise Exception(f\"{label} is not a word in the dataset\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmth0snBS_Rw"
      },
      "source": [
        "## Function definitions\n",
        "Functions to download and process or data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM3XdYvtM3se"
      },
      "source": [
        "# From: https://github.com/tensorflow/tensorflow/blob/de034a2911bdf0547a92e79b0c9858f8c4625fe0/tensorflow/examples/speech_commands/input_data.py\n",
        "def maybe_download_and_extract_dataset(data_url, dest_directory):\n",
        "  \"\"\"Download and extract data set tar file.\n",
        "  If the data set we're using doesn't already exist, this function\n",
        "  downloads it from the TensorFlow.org website and unpacks it into a\n",
        "  directory.\n",
        "  If the data_url is none, don't download anything and expect the data\n",
        "  directory to contain the correct files already.\n",
        "  Args:\n",
        "    data_url: Web location of the tar file containing the data set.\n",
        "    dest_directory: File path to extract data to.\n",
        "  \"\"\"\n",
        "  if not data_url:\n",
        "    return\n",
        "  if not os.path.isdir(dest_directory):\n",
        "    os.makedirs(dest_directory)\n",
        "  file_name = data_url.split('/')[-1]\n",
        "  file_path = os.path.join(dest_directory, file_name)\n",
        "\n",
        "  if not os.path.exists(file_path):\n",
        "    def _progress(count, block_size, total_size):\n",
        "      sys.stdout.write(\n",
        "        '\\r>> Downloading %s %.1f%%' %\n",
        "        (file_name, float(count * block_size) / float(total_size) * 100.0))\n",
        "      sys.stdout.flush()\n",
        "\n",
        "    try:\n",
        "      file_path, _ = urllib.request.urlretrieve(data_url, file_path, _progress)\n",
        "    except:\n",
        "      print(\n",
        "        'Failed to download URL: {0} to folder: {1}. Please make sure you '\n",
        "        'have enough free space and an internet connection'.format(\n",
        "          data_url, file_path))\n",
        "      raise\n",
        "    print()\n",
        "    statinfo = os.stat(file_path)\n",
        "    print('Successfully downloaded {0} ({1} bytes)'.format(\n",
        "      file_name, statinfo.st_size))\n",
        "    tarfile.open(file_path, 'r:gz').extractall(dest_directory)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WFnzucQLW_k"
      },
      "source": [
        "def _generate_silence(file, num):\n",
        "  data, sr = sf.read(file)\n",
        "  split = []\n",
        "  duration = int(np.ceil(len(data) / sr))\n",
        "\n",
        "  out_path = os.path.join(OUT_DIR, 'silence')\n",
        "  if not os.path.isdir(out_path):\n",
        "    os.mkdir(out_path)\n",
        "    \n",
        "  for i in range(num):\n",
        "    o = math.floor(np.random.uniform(0, duration-1))\n",
        "    temp = data[o * sr: o * sr + sr]\n",
        "    temp *= np.random.uniform(0, 1)\n",
        "    if '_noise_' in file:\n",
        "      temp *= 0.1\n",
        "    split.append(temp)\n",
        "\n",
        "  base_name = os.path.splitext(os.path.basename(file))[0]\n",
        "  for i in range(num):\n",
        "    filename = os.path.join(out_path, f'{base_name}_{i}.wav')\n",
        "    sf.write(filename, split[i], sr)\n",
        "\n",
        "\n",
        "\n",
        "def _write_files(labeled_files, unknown_files, noise_files):\n",
        "  if os.path.isdir(OUT_DIR):\n",
        "    shutil.rmtree(OUT_DIR)\n",
        "  os.mkdir(OUT_DIR)\n",
        "  \n",
        "  for index, files in enumerate(labeled_files):\n",
        "    label = SELECTED_WORDS[index]\n",
        "    label_path = os.path.join(OUT_DIR, label)\n",
        "    os.mkdir(label_path)\n",
        "    for file_path in files:\n",
        "      shutil.copy2(file_path, f'{label_path}/')\n",
        "  \n",
        "  unknown_path = os.path.join(OUT_DIR, 'unknown')\n",
        "  os.mkdir(unknown_path)\n",
        "\n",
        "  for file_path in unknown_files:\n",
        "    shutil.copy2(file_path, unknown_path)\n",
        "\n",
        "  num_slices_per_noise_file = math.floor(len(unknown_files) / len(noise_files))\n",
        "  print('num_slices_per_noise_file',num_slices_per_noise_file)\n",
        "  for noise_file in noise_files:\n",
        "    _generate_silence(noise_file, num_slices_per_noise_file)\n",
        "\n",
        "  \n",
        "def _get_wav_files(label):\n",
        "  dir = os.path.join(DOWNLOAD_DIR, label)\n",
        "  files = []\n",
        "  for file in os.listdir(dir):\n",
        "    if file.endswith(\".wav\"):\n",
        "        files.append(os.path.join(DOWNLOAD_DIR, label, file))\n",
        "  return files\n",
        "\n",
        "def _check_label(label):\n",
        "  if not label in ALL_WORDS:\n",
        "    raise Exception(f\"{label} is not a word in the dataset\")\n",
        "\n",
        "def generate():\n",
        "  for label in SELECTED_WORDS:\n",
        "    _check_label(label)\n",
        "  \n",
        "  np.random.seed(0)\n",
        "  \n",
        "  list_of_files = [_get_wav_files(label) for label in SELECTED_WORDS]\n",
        "  if MAX_FILES > 0:\n",
        "    max_per_label = math.floor(MAX_FILES / (len(SELECTED_WORDS)+2))\n",
        "    list_of_files = [np.random.choice(l, max_per_label, replace=False) for l in list_of_files]\n",
        "    \n",
        "  num_word_files = len(list_of_files[0])\n",
        "\n",
        "  list_of_non_words = [word for word in ALL_WORDS if not word in SELECTED_WORDS]\n",
        "  list_of_non_word_files = np.hstack([_get_wav_files(label) for label in list_of_non_words])\n",
        "\n",
        "  random_selection = np.random.choice(list_of_non_word_files, min(num_word_files, len(list_of_non_word_files)), replace=False)  \n",
        "\n",
        "  noise_files = _get_wav_files('_background_noise_')\n",
        "\n",
        "  _write_files(list_of_files, random_selection, noise_files)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtWCa6J-TE9I"
      },
      "source": [
        "# Download dataset and process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezt6vFepMoS6"
      },
      "source": [
        "maybe_download_and_extract_dataset(DATA_URL, DOWNLOAD_DIR)\n",
        "generate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E167WAeJNcNI"
      },
      "source": [
        "!zip -r /content/my_voice_dataset.zip $OUT_DIR\n",
        "from google.colab import files\n",
        "files.download(\"/content/my_voice_dataset.zip\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}