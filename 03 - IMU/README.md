# Gesture Classification using TF Lite 4 Microcontrollers and the IMU sensor
This experiment is in three parts – data collection, model training and deployment.

## Preparation
First make sure you have the necessary libraries installed. For the IMU sensors on the Arduino Nano 33 BLE Sense board, we need to install the LSM9DS1 library.
1. Go to the library manager (`Tools->Library Manager`) and search for LSM9DS1. 
1. Click install
1. While you're at it, make sure you have *Tensorflow Lite for Microcontrollers* installed too. You should have if you did the previous exercise, but just in case - search for *"Tensorflow"* and install the library published by *Pete Warden*
1. Click install

## Data Collection
We're going to record some "gestures" by collecting data from the onboard IMU accelerometer and gyroscope.

1. Open the sketch `03a_collect_imu_data/03a_collect_imu_data.ino`.
1. Have a look at the code, especially the top where various configuration variables are defined. You can tweak them if you like if you discover the capturing is too short / long or too sensitive / insensitive
1. Compile and upload the sketch by pressing the arrow button.
1. Once the sketch is running, open the Serial Monitor (`Tools->Serial Monitor`) and familiarize yourself with how the sketch works. You can also look in the Plotting Monitor (`Tools->Serial Plotter`) to see a graph representation of the data.
1. Recording process:
    1. Clear the Serial Monitor (*press the clear button*)
    1. Reset the Arduino by pressing once quickly on the onboard button.
    1. Wait for the Serial Monitor to reconnect and show you the output `ax,ay,az,gx,gy,gz`.
    1. Make some recordings by holding the Arduino in your hand and moving your arm.
    1. The onboard LED will show the following states:
        * Green - Ready to record
        * Red - Recording
        * Blue - Waiting for the board to be still again
    1. Do the same gesture about 20-30 times.
    1. Copy the content of the Serial Monitor to a file and save it under the name `<gesture>.csv` where `<gesture>` is the name you want to give this particular gesture.
    1. Do the same thing over from step 1. for each gesture you want to record. I suggest starting small, maybe just two or three gestures.

## Train the model
Open this Colab and follow the instructions

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/rikard-io/ea8e0b166dd5e30ee5240868e434b5e7/itp-creating-with-tinyml-imu.ipynb)


## Test it on.
1. Put the content of your `model.h` file generated by the colab inside of `03b_imu_classification/model.h`. 
1. Update the gesture / classes and any other capturing properties you might have changes in `03b_imu_classification/03b_imu_classification.ino`
1. Build and compile the sketch
1. Open the Serial Monitor and test it out.
1. Test your gestures.
1. If it doesn't perform as you expected, try improving it by collecting the data. Try to get a feel for when the model gets confused and try capturing more data in that situation.
1. If you collect more data - copy everything **except** the header `'ax,ay,az,gx,gy,gz'` from the Serial Monitor to the bottom of your previous csv files.

## Going further
These are optional explorations you can do:
1. Add more gestures. How many can you add and still have it reliably classify them? How different do they need to be? - You might have to try and modify the model to improve performance.
1. Make shorter gestures (modify the `num_samples_per_gesture` in both the capture and the classifier sketch). What effect does that have on Accuracy and responsiveness?
1. Connect the gesture to something more meaningful than just printing to the console – there are USB keyboard , USB MIDI, BLE and many more libraries for the Arduino Nano 33 Sense that you can use. Maybe make an interface for a P5js sketch – check out [p5.ble.js](https://itpnyu.github.io/p5ble-website/)).
1. Experiment with different Neural Network architectures. Try change number of neurons and layers and see what effect it has on your training.